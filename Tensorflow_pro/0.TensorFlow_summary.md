

```python

# coding:utf-8

import tensorflow as tf

g1 = tf.Graph()
with g1.as_default():
	v = tf.get_variable("v",
		initializer = tf.zeros_initializer(shapes=[1]))


g2 = tf.Graph()
with g2.as_default():
	v = tf.get_variable("v",
		initializer = tf.ones_initializer(shapes=[1]))

with tf.Session(graph=g1) as sess:
	tf.initialize_all_variables().run()
	with tf.variable_scope("",reuse=True):
		print(sess.run(tf.get_variable("v")))

with tf.Session(graph=g2) as sess:
	tf.initialize_all_variables().run()
	with tf.variable_scope("",reuse=True):
		print(sess.run(tf.get_variable("v")))

# for GPU device
g = tf.Graph()
with g.device('/gpu:0'):
	result = a+b
```

有效整理tensorflow程序中的资源是计算图中一个重要功能，在一个计算图中，通过集合（collection）来管理不同类别的资源
`tf.add_to_collection`将资源加入一个或多个集合中`tf.get_collection`获取一个集合中所有资源

### 数据模型-张量tensor

数据的表示形式，从功能角度来讲，张量tensor可以被理解成多维数组。零阶张量tensor表示一个数，一阶张量tensor为向量，也就是一维数组；n阶张量tensor可以理解成一个n维数组。


```python
import tensorflow as tf
a = tf.constant([1.0,2.0],name="a")
b = tf.constant([2.0,4.0],name="a")
result = tf.add(a,b,name="sum")
print(result)
```
输出结果
`Tensor("sum:0", shape=(2,), dtype=float32)`
可见张量(tensor)与Numpy中的数组不同，TensorFlow计算的结果不是一个具体数字，而是一个张量结构。每个张量(tensor)保存了三个属性：名字name,维度shape,类型type

#### 张量(tensor)两大用途

 - 对中间计算的引用。当个计算包含很多中间结果时，使用张量(tensor)可以提高代码可读性。
 - 计算图构造完成后，张量(tensor)可以用来获得计算结果。如上述代码中`tf.Session.run(result)`来获得结果。

### 运行模型-会话(Session)

```python
# coding:utf-8
# 创建一个会话，并通过python上下文管理器来管理这个会话
with tf.Session() as sess:
	sess.run(...)
	sess.close()
```
用TensorFlow实现一个神经网络
参考之前我的博文[用Tensflow写简单的神经网络](https://ulsonhu.github.io/%E7%94%A8Tensflow%E5%86%99%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html)



<img width="90%" height="90%" src="/uploads/nn_playground.png">
<div align="center">两层神经网络</div>

如图最右边提供四个数据集已测试神经网络，FEATURE表示数据特征，通过对数据的特征提取得到的特征向量（feature vector)。下一步，特征向量作为神经网络的输入，神经网络主体结构则显示为图中间位置,即两层隐藏层。在二分类问题中，输出层通常包含一层，而这个节点输出一个实数值。




### 总结：训练神经网络的过程可分为以下3个步骤
1. 定义神经网络的结构和FeedBack的输出结果
2. 定义损失函数(Cost Function)和优化算法(AdamOptimize,Back Propagation)
3. 生成会话(Session)并在训练集上反复运算





























